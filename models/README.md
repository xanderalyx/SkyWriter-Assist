# ðŸ¤– models/

This directory serves as the artifact storage for the trained Machine Learning models and the full set of evaluation results generated by the `src/train_from_merged.py` script.

## 1. Trained Artifacts

The following serialised Python objects (`.pkl` files) are stored here:

* **`SVC_BEST_SVC_C100_gesture_model.pkl`**: The **final, deployed Support Vector Classifier (SVC) model**. This model demonstrated the highest performance (based on Macro F1-Score and low confusion) and is loaded by `src/realtime_predictor.py` for live gesture classification.
* **`scaler.pkl`**: The **StandardScaler** object fitted to the training data. This must be loaded and applied to new, live feature vectors before they are passed to the classifier to ensure feature normalisation is consistent.
* **`DecisionTreeClassifier_gesture_model.pkl`**: Trained model artifact from the Decision Tree evaluation run.
* **`KNeighborsClassifier_gesture_model.pkl`**: Trained model artifact from the K-Nearest Neighbors evaluation run.
* **`RandomForestClassifier_gesture_model.pkl`**: Trained model artifact from the Random Forest evaluation run.

## 2. Evaluation Results and Metrics

These image files provide the visual evidence used to compare and select the best model :

* **`model_comparison_chart.png`**: A chart summarising the performance metrics (Accuracy, F1-Score, Precision, Recall) across all evaluated models (SVC, Random Forest, Decision Tree, KNN).
* **`*confusion_matrix.png`**: A confusion matrix is generated for each major model variant (e.g., SVC\_C1, SVC\_C10, Decision Tree). These heatmaps were crucial for identifying **confusable gesture pairs** (e.g., distinguishing 'P' from 'R') and determining which model generalises best across all 11 classes.

## 3. Model Selection Criterion

The final $\text{SVC}$ model (C=100) was chosen because it offered the best balance of:

1.  **High Macro F1-Score:** Ensuring strong performance across **all 11 gesture classes**.
2.  **Minimal Confusion:** Its confusion matrix showed the lowest number of misclassifications between highly similar gestures.

This approach prioritises robust, consistent performance over simple, overall accuracy.
